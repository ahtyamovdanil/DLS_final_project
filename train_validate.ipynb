{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import imp\n",
    "import TransformerTrainer\n",
    "import MyTransformer\n",
    "from TransformerTrainer import MyTranslator\n",
    "from DataModule import BaseDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import random\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "%env CUBLAS_WORKSPACE_CONFIG :16:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 50\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_deterministic(True)\n",
    "\n",
    "data_module = BaseDataModule(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device = DEVICE,\n",
    "    data_path=\"./data/eng_rus.txt\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"src_vocab_size\": data_module.src_vocab_len,\n",
    "    \"trg_vocab_size\": data_module.trg_vocab_len,\n",
    "    \"d_model\": 512,\n",
    "    \"n_enc_layers\": 6,\n",
    "    \"n_dec_layers\": 6,\n",
    "    \"n_enc_heads\": 8,\n",
    "    \"n_dec_heads\": 8,\n",
    "    \"enc_dropout\": 0.1,\n",
    "    \"dec_dropout\": 0.1,\n",
    "    \"src_pad_idx\": data_module.src_pad_idx,\n",
    "    \"trg_pad_idx\": data_module.trg_pad_idx\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.070626 млн\n"
     ]
    }
   ],
   "source": [
    "plmodel = MyTranslator(**params)\n",
    "plmodel.to(DEVICE)\n",
    "\n",
    "num_params = sum(p.numel() for p in plmodel.parameters() if p.requires_grad)\n",
    "print(f\"{num_params/1e6} млн\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | model     | Transformer      | 57.1 M\n",
      "-----------------------------------------------\n",
      "57.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "57.1 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d519fbeae68492ca0dcdc632d3d33de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c69e1c1ee9648e4bc9f28aae0c64b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b598c76212fa4c60ae6ef41ba4722b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef0defa65034dcba30da981b083f014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7281a9c178c54308a288a77e1b1722a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad77220df8f45888e07025b275602da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe4e096db764f3b9a0ff8921742138f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaac3288672493aaffcfb689fc11141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daca4e0244ea4776b854323e33a60229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac9ae95a12849f0bd855b7e203dd719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4143b0cef3fd42ec845c93f21124acb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d954dd3cea464bd38f0eab67bf8adc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349e0b11e1be4f0b80746ccd13b5d15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b407a6b8704da6b9dbdde6fcdd787f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 12\n",
    "CLIP = 1\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('./logs/')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='avg_val_loss',\n",
    "   min_delta=0.01,\n",
    "   patience=2,\n",
    "   verbose=False,\n",
    "   mode='mean'\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gradient_clip_val=CLIP,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    callbacks=[early_stop_callback, lr_monitor], \n",
    "    logger=tb_logger,\n",
    "    log_every_n_steps=20\n",
    ")\n",
    "data_module.setup('fit')\n",
    "trainer.fit(plmodel, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"models/myTransformer3.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(plmodel.model.state_dict(), 'models/transformer_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MyTranslator(\n  (criterion): CrossEntropyLoss()\n  (model): Transformer(\n    (encoder): Encoder(\n      (embed): Embedding(6736, 512)\n      (pe): PositionalEncoder(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (layers): ModuleList(\n        (0): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n        (1): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n        (2): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n        (3): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n        (4): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n        (5): EncoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (attn): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): Norm()\n    )\n    (decoder): Decoder(\n      (embed): Embedding(9250, 512)\n      (pe): PositionalEncoder(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (layers): ModuleList(\n        (0): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (1): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (2): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (3): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (4): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (5): DecoderLayer(\n          (norm_1): Norm()\n          (norm_2): Norm()\n          (norm_3): Norm()\n          (dropout_1): Dropout(p=0.1, inplace=False)\n          (dropout_2): Dropout(p=0.1, inplace=False)\n          (dropout_3): Dropout(p=0.1, inplace=False)\n          (attn_1): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (attn_2): MultiHeadAttention(\n            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n            (attention): PosAttentionLayer(\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (ff): FeedForward(\n            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n      )\n      (norm): Norm()\n    )\n    (out): Linear(in_features=512, out_features=9250, bias=True)\n  )\n)"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyTranslator.load_from_checkpoint(checkpoint_path=\"models/myTransformer3.ckpt\", **params)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "source": [
    "## Посмотрим, как обученный трансформер справляется с переводом"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nearest airport is vnukovo international airport , 23 km from apartment clubapart on studenchenskaya 16 .\n",
      "расстояние до международного аэропорта внуково от апартаментов « clubapart на студенческой , 16 » составляет 23 км .\n",
      "расстояние от апартаментов до международного аэропорта внуково составляет 23 , 7 км .\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "src = data_module.test_iter[idx].src\n",
    "trg = data_module.test_iter[idx].trg\n",
    "translation = utils.translate_sentence(data_module.test_iter[idx].src, model.model, data_module.src_field, data_module.trg_field, 80, DEVICE)\n",
    "print(\" \".join(src))\n",
    "print(\" \".join(trg))\n",
    "print(\" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.26132895673405926\n"
     ]
    }
   ],
   "source": [
    "utils.calculate_bleu(\n",
    "    data = data_module.test_iter, \n",
    "    src_field = data_module.src_field, \n",
    "    trg_field = data_module.trg_field,\n",
    "    model = model.model,\n",
    "    device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}